---
title: "agds_report1_norabisang"
output: html_document
---
setup of libraries
```{r}
library(keyring)
library(appeears)
library(sf)
library(here)
library(terra)
library(ggplot2)
library(leaflet)
library(htmlwidgets)
library(phenocamr)
library(dplyr)

```
The phenocamr R package uses the PhenoCam API to access the latest GCC time series and derive phenology using a threshold-based methodology similar to that described in Chapter 5. The data in Figure 6.1 should therefore be familiar. The phenocamr API call also downloads DAYMET data, which includes both daily minimum and maximum data. This ancillary data will be used in this basic modelling example below.
```{r}
# download greenness time series,
# calculate phenology (phenophases),
# amend with DAYMET data
phenocamr::download_phenocam(
  site = "harvard$",
  veg_type = "DB",
  roi_id = "1000",
  daymet = TRUE,
  phenophase = TRUE,
  trim = 2022,
  out_dir = tempdir()
  )

harvard_phenocam_data <- readr::read_csv(
  file.path(tempdir(), "harvard_DB_1000_3day.csv"), 
  comment = "#"
  )

# reading in harvard phenology only retaining
# spring (rising) phenology for the GCC 90th
# percentile time series (the default)
harvard_phenology <- readr::read_csv(
  file.path(
    tempdir(),
    "harvard_DB_1000_3day_transition_dates.csv"
    ),
  comment = "#"
) |>
  dplyr::filter(
    direction == "rising",
    gcc_value == "gcc_90"
  )
```
Growing degree days are defined as the cumulative sum of temperatures above a specified threshold (T0 , most commonly T0= 5°C).
Now we implement the GDD. 
```{r}
# return mean daily temperature as well
# as formal dates (for plotting)
harvard_temp <- harvard_phenocam_data |>
  group_by(year) |>
  dplyr::mutate(
    tmean = (tmax..deg.c. + tmin..deg.c.)/2
  ) |> 
  dplyr::mutate(
    date = as.Date(date),
    gdd = cumsum(ifelse(tmean >= 5, tmean - 5, 0))
  ) |>
  dplyr::select(
    date,
    year,
    tmean,
    gdd
  ) |>
  ungroup()

# convert the harvard phenology data and only
# retain required data
harvard_phenology <- harvard_phenology |>
  mutate(
    doy = as.numeric(format(as.Date(transition_25),"%j")),
    year = as.numeric(format(as.Date(transition_25),"%Y"))
  ) |>
  select(
    year,
    doy,
    transition_25,
    threshold_25
    )
```

Our GDD-based leaf-out model can be written in the form of a function that takes the temperature time series as its first argument, and as two parameters the temperature threshold above which temperatures are accumulated, and the critical GDD that determines the DOY at which leaf-out is predicted.

```{r}
gdd_model <- function(temp, par) {
  # split out parameters from a simple
  # vector of parameter values
  temp_threshold <- par[1]
  gdd_crit <- par[2]
  
  # accumulate growing degree days for
  # temperature data
  gdd <- cumsum(ifelse(temp > temp_threshold, temp - temp_threshold, 0))
  
  # figure out when the number of growing
  # degree days exceeds the minimum value
  # required for leaf development, only
  # return the first value
  doy <- unlist(which(gdd >= gdd_crit)[1])
  
  return(doy)
}
```

Running the model on the original 2010 data with the previously observed values and parameters 5°C for the temperature threshold and 130.44°C for the critical GDD, should yield a predicted leaf-out date that matches the observed leaf-out date of 114.

```{r}
# confirm that the model function
# returns expected results (i.e. DOY 114)
# (we filter out the year 2010, but
# removing the filter would run the
# model for all years!)
prediction <- harvard_temp |>
  dplyr::filter(
    year == 2010
  ) |>
  group_by(year) |>
  summarize(
    pred = gdd_model(
      temp = tmean,
      par = c(5, 130.44)
    )  
  )

print(prediction)
```

The optimization minimizes a cost function which is defined as the root mean squared error (RMSE) between the observed (observed Day of phenology start?) and the predicted values (predicted Day of phenology start?).

```{r}
# run model and compare to true values
# returns the RMSE
rmse_gdd <- function(par, data) {
  
  # split out data
  drivers <- data$drivers
  validation <- data$validation
  
  # calculate phenology predictions
  # and put in a data frame
  predictions <- drivers |>
    group_by(year) |>
    summarise(
      predictions = gdd_model(
        temp = tmean,
        par = par
      )
    )
  
  predictions <- left_join(predictions, validation, by = "year")
  
  rmse <- predictions |>
    summarise(
      rmse = sqrt(mean((predictions - doy)^2, na.rm = TRUE))
    ) |>
    pull(rmse)
  
  # return rmse value
  return(rmse)
}

```
 
Limitation of parameter space to reduce computation, as temperature thresholds fall within a range that is determined by physiological limits of plant activity.
This is the optimization routine (calling the cost function 4000 times) the optimal parameters for the temperature threshold and number of accumulation days respectively will be determined.

```{r}
# starting model parameters
par = c(0, 130)

# limits to the parameter space
lower <- c(-10,0)
upper <- c(45,500)

# data needs to be provided in a consistent
# single data file, a nested data structure
# will therefore accept non standard data formats
data <- list(
  drivers = harvard_temp,
  validation = harvard_phenology
  )

# optimize the model parameters
optim_par = GenSA::GenSA(
 par = par,
 fn = rmse_gdd,
 lower = lower,
 upper = upper,
 control = list(
   max.call = 4000
   ),
 data = data
)$par
```
These optimal parameters can now be plugged back into the model and run across all available years. 
```{r}
# run the model for all years
# to get the phenology predictions
predictions <- harvard_temp |>
  group_by(year) |>
  summarize(
   prediction = gdd_model(
    temp = tmean,
    par = optim_par
  )  
  )
```
A region around Boston in the eastern United States will be used, defined by the coordinates -72 to -70 East, and 42 to 44 North. 
The DAYMET raster data to scale the results spatially (relying on the same driver data as used during parameterization) will be used. First both minimum and maximum temperature data will be downloaded and averaged to a mean daily value, using the appeears R package (Hufkens 2023).

```{r}
library(ncdf4)
local_file <- tempfile(fileext = ".nc")
download.file("https://github.com/fabern/handfull_of_pixels/raw/refs/heads/main/data/DAYMET.004_2012/DAYMET.004_1km_aid0001.nc",
              destfile = local_file,
              mode="wb")
r1 <- terra::rast(local_file)
# Assign the correct CRS, e.g. WGS84 (EPSG:4326)
terra::crs(r1) <- "epsg:4326"

# Calculate the daily mean values based on 'tmin' and 'tmax'
mean_layer <- terra::mean(r1["tmax"], 
                          r1["tmin"])
# fix the variable naming
varnames(mean_layer) <- "tmean"                             
names(mean_layer) <- gsub("tmax","tmean",names(mean_layer))
```


Definition of a path inside project.
Download of the file directly to project folder. 
After that loading of the Data. 
```{r}
dest_file <- here::here("data-raw", "DAYMET_2012_aid0001.nc")
  
download.file(
    "https://github.com/fabern/handfull_of_pixels/raw/refs/heads/main/data/DAYMET.004_2012/DAYMET.004_1km_aid0001.nc",
    destfile = dest_file,
    mode = "wb"   # important for binary files like .nc
  )
r1 <- terra::rast(dest_file)  # Load with terra
terra::crs(r1) <- "epsg:4326" # Assign CRS

r1["tmax"]   #for variable accessing
r1["tmin"]
  

```
Now define names and subset the first 180 days to XY
```{r}
 # Calculate the daily mean values based on 'tmin' and 'tmax'
library(terra)  
mean_layer <- terra::mean(r1["tmax"], 
                            r1["tmin"])
  # fix the variable naming
  # subset to first 180 days
  ma_nh_temp <- terra::subset(
    mean_layer,
    1:180
  )

varnames(mean_layer) <- "tmean"                             
names(mean_layer) <- gsub("tmax","tmean",names(mean_layer))
terra::plot(mean_layer)
  
```
 Limitation the data to the first 180 days (layers) of the dataset to reduce the memory footprint of the calculations.

```{r}
# subset to first 180 days
ma_nh_temp <- terra::subset(
  mean_layer,
  1:180
)
```
Appliance of the model to this raster (cube) using the the terra::app() function and an appropriately formulated function, i.e. our growing degree day model gdd_model()

```{r}
predicted_phenology <- terra::app(
  ma_nh_temp,
  fun = gdd_model,
  par = optim_par
)
```

Plotting of the predicted phenology. 
Will result in an interactive map of the spatially scaled optimized growing degree model using DAYMET daily mean temperature data for tile 11935, including the greater Boston area in the south-east to the White Mountains in the north-west.
```{r}
library(leaflet)

# set te colour scale manually
pal <- colorNumeric(
  "magma",
  values(predicted_phenology),
  na.color = "transparent"
  )

# build the leaflet map
# using ESRI tile servers
# and the loaded demo raster
leaflet() |> 
  addProviderTiles(providers$Esri.WorldImagery, group = "World Imagery") |>
  addProviderTiles(providers$Esri.WorldTopoMap, group = "World Topo") |>
  addRasterImage(
    predicted_phenology,
    colors = pal,
    opacity = 0.8,
    group = "Phenology model results"
    ) |>
  addLayersControl(
    baseGroups = c("World Imagery","World Topo"),
    position = "topleft",
    options = layersControlOptions(collapsed = FALSE),
    overlayGroups = c("Phenology model results")
    ) |>
  addLegend(
    pal = pal,
    values = values(predicted_phenology),
    title = "DOY")
```


Exercize Questions

1) How can you improve the model used to regionally scale the results in Chapter 6?
Provide at least three ways to improve the model used.
 
  1.1)Cross-validation across years (temporal) or across space (space). This would check the       generaliseability of the GDD model. Instead of fitting parameters to all data                simultaneously, this would leave one year/site out, optimize the model on the rest of        the data, and then predict the set that has been left out.
      If the model predicts well over all repetitions, this suggests that it captures true         biological relationships rather than noise (->Overfitting!)
  

  1.2) Smoothing of the temperature (with eg Savitzky–Golay)
       A smoothing window (of eg. 3days) could be used before accumulation of GDD. This would        make our GDD less sensitive to isolated single-day warm or cold spikes, which are            not very likely to shift buddburst substantially, reducinv bias in early or                 late months of the year. At the same time the temperature rise dynamics would still be       preserved. Further smoothed temperature time series would lead to smoother residuals,        which would make downstream regression or kNN corrections more stable and less       sensitive to random variance
    

  1.3) Our GDD model assumes that each day with Temperature mean >5°C contributes equally, in reality though plant development could also accelerate or plateau. We could apply K-Nearest Neighbour (As in chapter 10), which is really good at pattern recognition and non-linear relationships. These would be beneficial in our model as especially when not only cumulative growing days matter but the climate variability patterns have influence on the (like frost events or really warm periods) it would be able to learn the relationship from the data with no need for an assumption about the form. 
  
2)Implement at least one of these methods
I implement the smoothing with the Savitzky-Golay-Filter used in chapter 5.3. 
```{r}
#install.packages("signal")
library(signal)

 NA %in% harvard_temp #no NA which is good :) else would return NAs and we could pre-fill missing days (with interpolation)

harvard_temp <- harvard_temp %>%
  group_by(year) %>%
  mutate(
    tmean_smooth = signal::sgolayfilt(tmean, p = 3, n = 5)  # smaller n for daily data
  )
#usually for GDD calculations a window of n=3-7 is used. I choose a 5-day moving window (2 days before and 2 after), which smooths out random daily noise but keeps the overall warming trend intact.
#Polynomial of 3 matches as can Capture slight “S”-shapes typical in temperature curves without overfitting.


harvard_temp <- harvard_temp %>%
  mutate(
    gdd = cumsum(ifelse(tmean_smooth >= 5, tmean_smooth - 5, 0))
  )


```

3)Statistically compare the results with the MODIS MCD12Q2 phenology product.
  3.1) compare the data spatially 
  3.2) describe why you might or might not see the same patterns
  3.3) consider that 2010 was a ‘special’ year for the north east of the US
  
  Loading of the MODIS phenology product. 
```{r}
# load libraries
library(geodata)

# download SRTM data
# This stores file srtm_38_03.tif in 
# subfolder elevation of tempdir()
geodata::elevation_3s(
    lat = 43,
    lon = -71,
    path = tempdir()
  )

# read the downloaded data
# use file.path() to combine

library(terra)
# My path is: 
dem_file <- file.path(tempdir(), "elevation", "srtm_22_04.tif")
# Loading of raster
dem <- rast(dem_file)
# Check if worked or no
dem
crs(dem)


```




```{r}
# load libraries
library(MODISTools)

#coordinates -72 to -70 East, and 42 to 44 North

# download and save phenology data
phenology <- MODISTools::mt_subset(
  product = "MCD12Q2",
  lat = 43,            #for coordinates I use midpoint of longitude/ latitude
  lon = -71,            
  band = "Greenup.Num_Modes_01",
  start = "2012-01-01",
  end = "2012-12-31",
  km_lr = 100,
  km_ab = 100,
  site_name = "Boston", #anpassen
  internal = TRUE,
  progress = FALSE
)
```

conversion of these integer values, counted from 1970, to day-of-year values (using as.Date() and format()). We only consider phenological events in the first 200 days of the year.

```{r}
# screening of data
phenology <- phenology |>
  mutate(
    value = ifelse(value > 32656, NA, value),
    value = as.numeric(format(as.Date("1970-01-01") + value, "%j")),
    value = ifelse (value < 200, value, NA)
  )

```

 
 conversion of tidy data to a geospatial (terra SpatRast) format

```{r}
phenology_raster <- MODISTools::mt_to_terra(
  phenology,
  reproject = TRUE
)

```


3.1
Spatial comparison that shows how the DOY differ between the prediction of the Model and the MODIS data. 
```{r}
library(terra)
library(ggplot2)
library(tidyterra)

# Ensure rasters align (resample GDD raster to MODIS grid if needed)
predicted_resampled <- terra::resample(predicted_phenology, phenology_raster, method = "near")

# Pixel-wise difference btw. DOY predicted by our model and the observed DOY from MODIS:
#positive ->model predicts later DOY than MODIS
#negative -> model predicts earlier DOY than MODIS
difference <- predicted_resampled - phenology_raster

# 1 Plot from MODIS observations
ggplot() +
  geom_spatraster(data = phenology_raster) +
  scale_fill_viridis_c(name = "MODIS DOY", na.value = NA) +
  theme_bw()

# 2 Plot of the  GDD predictions
ggplot() +
  geom_spatraster(data = predicted_resampled) +
  scale_fill_viridis_c(name = "GDD DOY", na.value = NA, option = "magma") +
  theme_bw()

# 3 Plot of difference (GDD - MODIS)
ggplot() +
  geom_spatraster(data = difference) +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0,
                       name = "GDD - MODIS DOY") +
  theme_bw()

# 4 Scatter plot of all pixels
df <- data.frame(
  MODIS = values(phenology_raster),
  GDD = values(predicted_resampled)
) %>% na.omit()


```
3.2) 
describe why you might or might not see the same patterns
What “speaks” for seeing similar patterns:
1) In both datasets the DOY is linked to Temperature, directly (model) with mean Temperature and indirectly (MODIS) with greening, which is temperature dependent. 
2) They both have information on /detect a spatial gradient considering latitude and longitude costal/inland -> climate gradients

What “speaks” for different patterns
1) MODIS can only detect early greening up not directly Temperature. 
2) MODIS can be distorted by cloud coverage, snow, sensor issues
3) The model does not consider that the vegetation could be very mixed and leaf-out could be dependent on other factors like day-length or precedent freezing events. 
4) Possible that the microclimate and topography are affecting DOY of leaf-out which is not considered in Model. 
5) The resolution of measuring might be different. 



 
3.3
2010 was a special year as the DOY for leaf-out was earlier than normal.Leaf-out requires the accumulation of a certain amount of heat (GDD), so warmer springs reach the threshold sooner and thus result in earlier greening up.
This indicates a warmer spring than normal. Below I calculated GDD accumulation  during early spring period (DOY 60–120) as I expect the biologically relevant window for bud development to lie in these days. Visible in the summary and plot are higher GDD values in 2010 and 2012 which indicate earlier spring warming that leads to earlier phenology and thus DOY for budd-out. In the mean temperature March-April is well visible that mean temperatures indeed were exceptionally high. 


```{r}
library(dplyr)
library(lubridate)

harvard_temp <- harvard_temp %>%
  mutate(doy = yday(date))   # create day-of-year column


spring_summary <- harvard_temp %>%
  filter(doy >= 60 & doy <= 120) %>%   # I choose days 60 -120 as this is ~spring 
  group_by(year) %>%
  summarize(
    mean_temp = mean(tmean, na.rm = TRUE),
    gdd_cum = sum(ifelse(tmean >= 5, tmean - 5, 0))
  )

spring_summary

#here as a visualization to see that temperature in 2010 was exceptionally warm as was in 2012
ggplot(spring_summary, aes(x = year)) +
  geom_line(aes(y = mean_temp), linewidth = 1, color = "limegreen") +
  geom_point(aes(y = mean_temp), color = "darkgreen") +
  labs(title = "Spring Temperature (march and april)",
       x = "year", y = "mean temperature (°C)") +
  theme_bw()





```








